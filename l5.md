# Проект: HA-прокси + ClickHouse 3×3 + ZooKeeper 3 ноды в серой сети

## Автоматизация развёртывания на Ansible (много ролей)

### Цель

С нуля **автоматизировать** (Ansible) подъем стенда:

* 2 узла `proxy` (Nginx) с VRRP-виртуальным IP (**keepalived**),
* 3 узла `zookeeper`,
* 9 узлов `clickhouse` (3 шарда × 3 реплики),
* локальный DNS (или управляемый `/etc/hosts`) в «серой» сети,
* **никакого внешнего интернета**; все артефакты, ключи, шаблоны, плейбуки — в репозитории.

### Результат

Один репозиторий, из которого:

* `make bootstrap` — подготовка узлов (ключи, python, факты),
* `make deploy` — полный деплой по ролям,
* `make test` — проверка работоспособности,
* `make fault-*` — инъекция отказов и проверка самовосстановления,
* `make destroy` — аккуратная очистка (по возможности).

---

## Сетевая модель (пример)

* подсеть: `10.10.0.0/24`
* VIP (VRRP): `10.10.0.100`
* proxy: `10.10.0.11` / `10.10.0.12`
* zk: `10.10.0.21` / `.22` / `.23`
* ch-s1: `10.10.0.31` / `.32` / `.33`
* ch-s2: `10.10.0.41` / `.42` / `.43`
* ch-s3: `10.10.0.51` / `.52` / `.53`

(имена в зоне `dc.local`: `proxy01.dc.local`, `zk02.dc.local`, `ch-s3-r2.dc.local`, и т.д.)

---

## Стек ролей (минимально необходимый)

1. `role_common`

   * timezone, locale, `apt`/`dnf` базовые пакеты, `python3`, `sudo` политика;
   * sshd: `PermitRootLogin no`, `PasswordAuthentication no`, `AllowUsers`;
   * firewall baseline (всё deny, нужные порты allow);
   * sysctl (только поддерживаемые ключи), `fstrim.timer` → включать только если поддерживается.

2. `role_dns` (один узел или все)

   * вариант A: `bind9` с зоной `dc.local`;
   * вариант B: `dnsmasq` (DHCP off, только DNS), хост-записи генерятся из инвентаря;
   * на всех хостах `resolv.conf` → указывает на внутренний DNS.

3. `role_keepalived` (на `proxy` узлах)

   * VRRP VIP с preempt; `virtual_router_id`, `priority master/backup`;
   * `notify_master`/`notify_backup` скрипты (логгировать смену роли);
   * протокол 112 открыт между проксями; gratuitous ARP включён.

4. `role_nginx_proxy`

   * Nginx OSS, `upstream` со всеми 9 CH-нODE: `max_fails=2` `fail_timeout=5s` `keepalive 64`;
   * `proxy_next_upstream error timeout http_502 http_503 http_504`;
   * access.log в **JSON** (поля: ts, req_id, remote, upstream_addr, status, rt, urt);
   * `server { listen 80; }` + `/healthz` (200).

5. `role_zookeeper` (3 ноды)

   * `zoo.cfg` с `server.1/2/3`; `myid` на каждой;
   * systemd unit; открыты порты 2181/2888/3888 для своих;
   * базовые проверки: `echo ruok | nc 2181` → `imok`.

6. `role_clickhouse` (9 нод)

   * установка из локальных пакетов/репозитория (без интернета);
   * конфиги: `config.d/zookeeper.xml`, `config.d/remote_servers.xml` (кластер 3×3), `users.d/*` (доступ по HTTP с auth);
   * `create_tables.sql`: `ReplicatedMergeTree('/clickhouse/tables/{shard}/t_demo','{replica}')`;
   * опционально `Distributed` поверх кластера;
   * хендлер на `SYSTEM RELOAD CONFIG`.

7. `role_firewall` (точечные правила)

   * proxy: open `80/tcp`, VRRP 112/PROTO;
   * zk: allow 2181/2888/3888 только из `zk*` и `ch*`;
   * ch: 8123/9000 только из `proxy*`/внутр. сети;
   * всё остальное drop; логирование в отдельную цепочку с rate-limit.

8. `role_checks` (самопроверки)

   * текстовые проверки, сохраняемые в `checks/*.txt`:

     * VIP миграция,
     * LB round-robin (20 запросов),
     * выпадение CH-ноды и восстановление,
     * потеря 1 ZK (работает), потеря 2 ZK (rw → ro),
     * `system.clusters`, `system.replicas` выдержки.

*(Можно объединить firewall в common, но отдельно — нагляднее.)*

---

## Оркестрация (порядок запуска, теги)

* `common` → `dns` → `firewall` (базовые allow для менеджмента) → `zookeeper` → `clickhouse` → `nginx_proxy` → `keepalived` → `checks`
* Глобальные теги: `common,dns,fw,zk,ch,nginx,keepalived,checks`.
* Rolling-подход для ClickHouse (батчами по реплике; шард за шардом).

---

## Репозиторий: рекомендуемая структура

```
ansible/
├── inventories/
│   ├── lab/hosts.ini
│   └── lab/group_vars/
│       ├── all.yml
│       ├── proxy.yml
│       ├── zookeeper.yml
│       └── clickhouse.yml
├── roles/
│   ├── role_common/
│   ├── role_dns/
│   ├── role_keepalived/
│   ├── role_nginx_proxy/
│   ├── role_zookeeper/
│   ├── role_clickhouse/
│   ├── role_firewall/
│   └── role_checks/
├── playbooks/
│   ├── site.yml
│   ├── 01_common.yml
│   ├── 02_dns.yml
│   ├── 03_firewall.yml
│   ├── 04_zookeeper.yml
│   ├── 05_clickhouse.yml
│   ├── 06_nginx.yml
│   ├── 07_keepalived.yml
│   └── 99_checks.yml
├── library/                  # свои action-плагины (если понадобятся)
├── filter_plugins/           # jinja-фильтры
├── molecule/                 # тестовые сценарии (опционально)
└── Makefile
```

### Пример `inventories/lab/hosts.ini`

```ini
[proxy]
proxy01 ansible_host=10.10.0.11
proxy02 ansible_host=10.10.0.12

[zookeeper]
zk01 ansible_host=10.10.0.21 myid=1
zk02 ansible_host=10.10.0.22 myid=2
zk03 ansible_host=10.10.0.23 myid=3

[clickhouse_s1]
ch-s1-r1 ansible_host=10.10.0.31 shard=1 replica=1
ch-s1-r2 ansible_host=10.10.0.32 shard=1 replica=2
ch-s1-r3 ansible_host=10.10.0.33 shard=1 replica=3

[clickhouse_s2]
ch-s2-r1 ansible_host=10.10.0.41 shard=2 replica=1
...

[clickhouse_s3]
...

[clickhouse:children]
clickhouse_s1
clickhouse_s2
clickhouse_s3

[all:vars]
ansible_user=admin
```

### Пример `group_vars/clickhouse.yml` (фрагменты)

```yaml
ch_http_port: 8123
ch_tcp_port: 9000
zk_quorum: [zk01.dc.local:2181, zk02.dc.local:2181, zk03.dc.local:2181]
cluster_name: cluster_3x3
```

---

## Примеры ключевых фрагментов

### Nginx upstream (role_nginx_proxy/templates/app.conf.j2)

```nginx
upstream clickhouse_backend {
{% for h in groups['clickhouse'] %}
    server {{ hostvars[h].inventory_hostname }}.dc.local:{{ ch_http_port|default(8123) }} max_fails=2 fail_timeout=5s;
{% endfor %}
    keepalive 64;
}

map $http_x_request_id $reqid { default $http_x_request_id; "" $request_id; }

log_format json escape=json
  '{ "ts":"$time_iso8601","remote":"$remote_addr","req":"$request","status":$status,
     "rt":$request_time,"urt":$upstream_response_time,"upstream":"$upstream_addr","req_id":"$reqid" }';

server {
    listen 80 default_server;
    server_name _;

    access_log /var/log/nginx/access.json json;
    error_log  /var/log/nginx/error.log warn;

    location /healthz { return 200; }

    location / {
        proxy_set_header Host              $host;
        proxy_set_header X-Real-IP         $remote_addr;
        proxy_set_header X-Forwarded-For   $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Request-Id      $reqid;

        proxy_http_version 1.1;
        proxy_set_header Connection "";

        proxy_next_upstream error timeout http_502 http_503 http_504;
        proxy_read_timeout 10s;
        proxy_send_timeout 10s;

        proxy_pass http://clickhouse_backend;
    }
}
```

### Keepalived (role_keepalived/templates/keepalived.conf.j2)

```conf
vrrp_instance VI_1 {
  state {{ keepalived_state }}           # MASTER / BACKUP
  interface {{ keepalived_interface }}
  virtual_router_id {{ keepalived_vrid }}
  priority {{ keepalived_priority }}     # MASTER > BACKUP
  advert_int 1
  authentication { auth_type PASS; auth_pass {{ keepalived_authpass }}; }
  virtual_ipaddress { {{ keepalived_vip }}/24 dev {{ keepalived_interface }} }
  garp_master_delay 1
  garp_master_repeat 3
  notify_master "/usr/local/bin/on_vip_master.sh"
  notify_backup "/usr/local/bin/on_vip_backup.sh"
}
```

### ZooKeeper (role_zookeeper/templates/zoo.cfg.j2)

```properties
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/var/lib/zookeeper
clientPort=2181
server.1=zk01.dc.local:2888:3888
server.2=zk02.dc.local:2888:3888
server.3=zk03.dc.local:2888:3888
```

### ClickHouse zookeeper.xml (role_clickhouse/templates/config.d/zookeeper.xml.j2)

```xml
<yandex>
  <zookeeper>
  {% for z in groups['zookeeper'] %}
    <node>
      <host>{{ hostvars[z].inventory_hostname }}.dc.local</host>
      <port>2181</port>
    </node>
  {% endfor %}
  </zookeeper>
</yandex>
```

### ClickHouse cluster (role_clickhouse/templates/config.d/remote_servers.xml.j2)

```xml
<yandex>
  <remote_servers>
    <{{ cluster_name }}>
    {% for shard_name in ['clickhouse_s1','clickhouse_s2','clickhouse_s3'] %}
      <shard>
        {% for h in groups[shard_name] %}
        <replica>
          <host>{{ hostvars[h].inventory_hostname }}.dc.local</host>
          <port>{{ ch_tcp_port|default(9000) }}</port>
        </replica>
        {% endfor %}
      </shard>
    {% endfor %}
    </{{ cluster_name }}>
  </remote_servers>
</yandex>
```

---

## Приёмочные тесты (роль `role_checks`)

Собираем **текстовые** артефакты в `ansible/checks/`:

1. **VIP failover**

   * Кто держит VIP, стоп master keepalived, проверка миграции VIP, старт назад.
   * Сохранить вывод команд → `vip-failover.txt`.

2. **LB поведение**

   * 20 запросов на VIP (`curl -s "http://VIP/?query=SELECT%201"`),
   * выдержки из `access.json` с `upstream`.
   * `proxy-lb.txt`.

3. **CH нода down/up**

   * `systemctl stop clickhouse-server` на одной ноде;
   * 10–20 запросов → нет 502/504, `upstream` ≠ down-нода;
   * старт обратно → снова появляется в логах.
   * `proxy-fail-node.txt`.

4. **ZK кворум**

   * stop 1 zk → INSERT/DDL проходит;
   * stop 2 zk → CH в read-only (ошибка записи), SELECT — ок;
   * восстановить.
   * `zk-quorum.txt`.

5. **CH-health**

   * `SELECT hostName(), cluster();` через VIP и напрямую на пару нод;
   * `system.clusters`, `system.replicas` с фильтром по кластеру.
   * `ch-health.txt`.

6. **Порты/маршруты/файрвол**

   * `ss -lntp | egrep ':(80|8123|9000|2181|2888|3888)\b'`;
   * `iptables-save`/`nft list ruleset` (обрезанные до сути).
   * `routes-ports.txt`.

---

## Качество автоматизации — обязательные требования

* **Идемпотентность**: повторный прогон `site.yml` → `changed=0 failed=0` (кроме осмысленных случаев).
* **Проверки до/после**: роли содержат проверки (например, поддержка DISCARD перед включением `fstrim.timer`), никаких «красивых падений» на пустом месте.
* **Теги**: каждая роль — со своим тегом, можно разворачивать частями.
* **Секреты**: только через Ansible Vault (если используются).
* **Безопасные `when`**: никакой записи в `/etc/hosts`, если `cloud-init manage_etc_hosts: true` (либо корректная правка шаблона).
* **Запрет устаревших модулей**: не использовать `apt_key` и прочий анахронизм — только keyrings + `signed-by=`.
* **Линт**: `ansible-lint`/`yamllint` зелёные.
* **Molecule** (опционально): сценарий для `role_nginx_proxy` и `role_keepalived` (basic converge).

---

## Makefile (пример)

```make
ANSIBLE=ansible-playbook -i inventories/lab/hosts.ini

bootstrap:
\t$(ANSIBLE) playbooks/01_common.yml --tags common -e 'bootstrap=true'

deploy:
\t$(ANSIBLE) playbooks/site.yml

deploy-core:
\t$(ANSIBLE) playbooks/01_common.yml playbooks/02_dns.yml playbooks/03_firewall.yml

deploy-backend:
\t$(ANSIBLE) playbooks/04_zookeeper.yml playbooks/05_clickhouse.yml

deploy-proxy:
\t$(ANSIBLE) playbooks/06_nginx.yml playbooks/07_keepalived.yml

test:
\t$(ANSIBLE) playbooks/99_checks.yml

fault-ch:
\t$(ANSIBLE) playbooks/fault_inject.yml --tags ch

fault-zk:
\t$(ANSIBLE) playbooks/fault_inject.yml --tags zk

destroy:
\t$(ANSIBLE) playbooks/destroy.yml
```

---

## Критерии оценки

* **Структура репозитория, читаемость и документация** — 10%
* **Идемпотентность / корректность ролей** — 20%
* **VRRP (переключение VIP), логика keepalived** — 15%
* **Nginx LB (пассивные health, json-логи, заголовки)** — 15%
* **ZooKeeper 3-ноды (кворум, сценарии отказов)** — 15%
* **ClickHouse 3×3 (кластер, реплики, базовые DDL/DML, `system.*`)** — 20%
* **Сетевая изоляция (firewall), строго «серая» сеть** — 5%

