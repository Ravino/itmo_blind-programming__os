# Отказоустойчивый HTTP-прокси для кластера ClickHouse (3×3) за VRRP-VIP + ZooKeeper (3 ноды)

## Цель

Собрать в «серой» сети стенд из:

* **двух HTTP-прокси на Nginx** с общим **keepalived VRRP VIP** (виртуальным IP) → HA на входе;
* кластера **ClickHouse: 3 шарда × 3 реплики** (всего 9 инстансов);
* кворумного **ZooKeeper (3 ноды)** для репликаций/метаданных CH.

Трафик пользователя → VIP → активная прокся → пул **живых** нод ClickHouse (HTTP 8123) → round-robin.
При падении активной прокси VIP **переключается** на вторую.
При падении CH-ноды — прокся **исключает** её из апстрима (пассивный health) и продолжает слать на оставшиеся; вернулась — снова в строю.
При потере 1 из 3 ZK — кластер работает; при потере 2 — ClickHouse уходит в **read-only** (ожидаемо).

Никакого внешнего интернета. Все IP из приватного пула. Все материалы — **только текстом** в репозитории (конфиги, юниты, скрипты, вывод команд для проверки).

---

## Топология (минимум)

* **Сегмент “серой” сети**: `10.10.0.0/24` (пример; выберите свой).
* **VIP (VRRP)**: `10.10.0.100/24` (пример).
* **Прокси**: `proxy01 (10.10.0.11)`, `proxy02 (10.10.0.12)` — Nginx + keepalived.
* **ZooKeeper**: `zk01 (10.10.0.21)`, `zk02 (10.10.0.22)`, `zk03 (10.10.0.23)`.
* **ClickHouse** (3 шарда × 3 реплики = 9 узлов), пример:

  * Шард 1: `ch-s1-r1 (10.10.0.31)`, `ch-s1-r2 (10.10.0.32)`, `ch-s1-r3 (10.10.0.33)`
  * Шард 2: `ch-s2-r1 (10.10.0.41)`, `ch-s2-r2 (10.10.0.42)`, `ch-s2-r3 (10.10.0.43)`
  * Шард 3: `ch-s3-r1 (10.10.0.51)`, `ch-s3-r2 (10.10.0.52)`, `ch-s3-r3 (10.10.0.53)`

DNS — локальный (из предыдущей работы) с зонами `dc.local`:

```
proxy01.dc.local → 10.10.0.11
proxy02.dc.local → 10.10.0.12
vip.dc.local     → 10.10.0.100
zk01/02/03.dc.local → 10.10.0.21/22/23
ch-sX-rY.dc.local → 10.10.0.3x/4x/5x
```

---

## Требования к поведению

### HA на входе

* На `proxy01/02` запускается **keepalived** (VRRP). VIP из приватного пула.
* При остановке/падении активной прокси VIP **перепрыгивает** на вторую ≤ 3–5 сек.
* Маска и ARP-announce настроены корректно (никаких “битых” ARP).

### Проксирование и health

* Nginx (**HTTP**) проксирует на пул CH-нод по `http://<host>:8123` (интерфейс ClickHouse HTTP).
* Балансировка: **round_robin** (по умолчанию).
* Здоровье апстрима: **пассивные** проверки (open-source Nginx):

  * `max_fails` и `fail_timeout` в `upstream`;
  * `proxy_next_upstream error timeout http_502 http_503 http_504`.
* При ошибках коннекта/ответа — сервер **временно исключается**; при следующей успешной попытке возвращается в пул (реальностью управляют настройки Nginx).
* **Опционально**: DNS-резолв апстримов с `resolver` + `resolve` на серверных именах, чтобы менять пул без рестарта (но это не обязательно, можно статикой).

### ClickHouse 3×3

* 9 инстансов ClickHouse с корректной конфигурацией `cluster.xml` (или `remote_servers`) на **3 шарда, в каждом 3 реплики**.
* На каждой ноде настроены **ReplicatedMergeTree** таблицы (демо-таблица) и/или **Distributed** таблицы.
  Логика здесь «проверочная»: важно, чтобы любой HTTP-запрос на 8123 к любой ноде ответил (например, `SELECT 1`, `SELECT hostName(), cluster()`), а реплики/шарды видны в `system.clusters`, `system.replicas`.
* **ZooKeeper** (3 ноды) прописан в конфиге ClickHouse. При падении 1 ноды ZK — чтение/запись работают; при падении 2 — CH переходит в **read-only** (ожидаемо).

### Изоляция сети

* Внешний мир недоступен. Разрешены только внутренние порты:

  * `proxy01/02`: 80 (и 8080 если нужно), VRRP (IP protocol 112) между проксями.
  * CH: 8123 (HTTP), 9000 (TCP) — **только** из «серой» сети (и/или только от проксей для 8123).
  * ZK: 2181/2888/3888 — только между `zk01-03` и CH-нодами.
* Запрет всего лишнего на вход (iptables/nftables), чёткие правила.

---

## Что именно нужно сделать (минимальный план)

### 1) Репозиторий

Структура (пример):

```
.
├── README.md
├── net/
│   ├── hosts.dc.local            # записи для /etc/hosts или dns-зона
│   └── firewall/                 # набор правил для узлов
│       ├── proxy01.rules
│       ├── proxy02.rules
│       ├── ch-*.rules
│       └── zk-*.rules
├── keepalived/
│   ├── proxy01-keepalived.conf
│   └── proxy02-keepalived.conf
├── nginx/
│   ├── upstream.conf             # upstream + сервер
│   ├── mime.types                # если надо
│   └── systemd/nginx.service     # если кастомный юнит
├── zookeeper/
│   ├── zoo.cfg
│   ├── myid.zk01
│   ├── myid.zk02
│   ├── myid.zk03
│   └── systemd/*.service
├── clickhouse/
│   ├── config.d/zookeeper.xml
│   ├── config.d/remote_servers.xml
│   ├── users.d/*.xml
│   ├── create_tables.sql         # демо-таблицы (ReplicatedMergeTree/Distributed)
│   └── systemd/clickhouse-server.service (если нужен)
└── checks/
    ├── vip-failover.txt          # вывод команд проверки VIP-переключения
    ├── proxy-lb.txt              # вывод 10–20 запросов на VIP → чередование upstream
    ├── proxy-fail-node.txt       # сценарий выпадения CH-ноды и восстановление
    ├── zk-quorum.txt             # сценарий падения 1, затем 2 ZK
    ├── ch-health.txt             # результаты простых SELECT и system.* таблиц
    └── routes-ports.txt          # netstat/ss/ip a/ip r/iptables-save выдержки
```

Все конфиги, юниты, скрипты — **текстом**. Никаких скриншотов.

### 2) Keepalived (VRRP)

* Две рольки (или просто два конфига), например:

  * `proxy01` — `state MASTER`, `priority 150`;
  * `proxy02` — `state BACKUP`, `priority 100`;
  * Общий `virtual_router_id`, `virtual_ipaddress = 10.10.0.100/24 dev <iface>`.
* Включить `garp_master_delay/garp_master_repeat` или `notify_*` скрипты по вкусу.
* В репозитории — конфиги и короткая памятка запуска.

### 3) Nginx как reverse-proxy к ClickHouse

* `upstream clickhouse_backend { ... }` c 9 серверами (`max_fails=2 fail_timeout=5s; keepalive 64;`).
* `server { listen 80; location / { proxy_pass http://clickhouse_backend; proxy_next_upstream error timeout http_502 http_503 http_504; ... } }`
* Логи — в JSON, чтобы было видно `upstream_addr` и статус.
* (Если хотите «соль» — добавьте `/ping` на Nginx, который не проксируется, для health-check VIP.)

### 4) ZooKeeper (3 ноды)

* `zoo.cfg`: server.1=zk01:2888:3888 и т.д., `tickTime`, `initLimit`, `syncLimit`, `dataDir`, `clientPort=2181`.
* На каждой ноде свой `myid`.
* Юниты systemd, запуск, базовая проверка `echo ruok | nc zk01 2181`.

### 5) ClickHouse 9×

* Конфиг `zookeeper.xml` на всех нодах (адреса трёх ZK).
* Конфиг `remote_servers.xml` (cluster 3×3).
* `users.d` — разрешить HTTP-доступ (минимально).
* Демо-таблицы:

  * `ReplicatedMergeTree('/clickhouse/tables/{shard}/t_demo','{replica}')` на каждой ноде;
  * `Distributed(cluster_3x3, db, t_demo, rand())` — где нужно.
* Проба: `SELECT hostName(), cluster()` на всех нодах; `system.replicas`, `system.clusters`.

### 6) Изоляция сети

* Файрволы на всех узлах, разрешающие только нужные порты/направления.
* В репозитории — экспорт правил (`iptables-save`/`nft list ruleset`).

---

## Проверки (все результаты — текстом; положить в `checks/`)

1. **VIP failover**

   * Показать `ip a` на обеих проксях (какой держит VIP).
   * Остановить keepalived на активной: `systemctl stop keepalived`.
   * Показать, что VIP появился на второй ноде.
   * Запустить назад; убедиться, что мастерство вернулось (если preempt).

   Сохранить команды и вывод → `checks/vip-failover.txt`.

2. **LB на Nginx**

   * 20 последовательных запросов на VIP:
     `for i in {1..20}; do curl -s http://10.10.0.100/?q=SELECT%201; done`
   * В ответах/логах должно быть видно **разные** upstream-адреса.
   * Выдержка из `access.log` (JSON) с полями `upstream_addr`, `status`.

   Скинуть → `checks/proxy-lb.txt`.

3. **Выпадение CH-ноды**

   * Остановить один из `ch-sX-rY`.
   * Сделать 10–20 запросов на VIP — ошибок 502/504 быть не должно, upstream адреса — только живые ноды.
   * Поднять ноду; показать, что она снова появляется в upstream (по логам видно).

   Скинуть → `checks/proxy-fail-node.txt`.

4. **ZK кворум**

   * Остановить 1 ZK → показать, что INSERT/CREATE (что угодно метадатное) работает.
   * Остановить вторую ZK → показать, что ClickHouse переходит в read-only (вставки/DDL не проходят; простые SELECT — ок).
   * Вернуть кворум → восстановление записи.

   Скинуть → `checks/zk-quorum.txt`.

5. **CH-сервис**

   * `SELECT hostName(), cluster();` с VIP (проксируется дальше).
   * Выдержки `system.clusters`, `system.replicas` (фильтр по вашему кластеру).

   Скинуть → `checks/ch-health.txt`.

6. **Сетевые порты/маршруты**

   * `ss -ltnp | grep -E ':(80|8123|9000|2181|2888|3888)\b'` на нужных.
   * Куски `iptables-save`/`nft list ruleset` (сократите до релевантного).

   Скинуть → `checks/routes-ports.txt`.

---

## README.md — что обязательно

* **Карта стенда** (кто где, какие IP/имена, VIP).
* **Порядок развертывания**: ZK → CH → Nginx → keepalived → firewall.
* **Как проверить**: отсылки на `checks/*.txt`.
* **Как «сломать» и что ожидается**: сценарии отказов (прокся, CH-нода, ZK).
* **Почему выбран именно пассивный health в Nginx** (у OSS нет активных health-checks; объяснить параметры `max_fails`, `fail_timeout`, `proxy_next_upstream`).

---

## Критерии оценки

* **VRRP/keepalived (20%)**: VIP стабильно мигрирует при отказе прокси; нет split-brain.
* **Nginx LB (25%)**: корректный round-robin, исключение «плохих» upstream, возврат «исцелившихся». Логи информативны (JSON).
* **ClickHouse 3×3 (25%)**: корректный cluster config, реплики/шарды, базовые запросы работают с любой ноды и через VIP.
* **ZooKeeper (20%)**: 3-нодный кворум, ожидаемая деградация в read-only при потере 2 нод, восстановление.
* **Сетевая гигиена (10%)**: закрыты лишние порты, всё только в «серой» сети, DNS/hosts настроены.

Бонусы:

* Сервисные `/healthz` на Nginx и/или простые HTTP-проверки на CH.
* Автоматический re-resolve upstream (DNS `resolve` + `resolver` в Nginx).
* Makefile с целями `deploy-*`, `test-*` для быстрой проверки.
* Скрипт «fault-injector» (останавливает/запускает ноды и собирает проверку в `checks/`).

---

## Подводные камни (обязательно проговорите в README)

* **Nginx OSS**: нет активных health-checks. Мы используем **пассивные** + `max_fails`/`fail_timeout`. Это значит: первая(ые) неудачная(ые) попытка(и) к мёртвой ноде будут, но дальше нода помечается как “временно down”.
* **CH смысл “любой ноде”**: для корректной шардовой логики используйте `Distributed` таблицы, иначе «любой ноде» может быть не тем, что вы ожидали. Для цели этой задачи достаточно корректно отвечающих HTTP-эндпоинтов.
* **ZK кворум**: при потере 2 из 3 — read-only поведения ClickHouse для DDL/записей — это **норма**.
* **VRRP в приватке**: не забываем про multicast/VRRP протокол 112 и ARP-анонсы.
* **Firewall**: сначала разрешения, потом запреты, чтобы не отрезать себе управление.


